# CSE234: Data Systems for Machine Learning


## 课程简介

- 所属大学：UCSD
- 先修要求：线性代数，深度学习，操作系统，计算机网络，分布式系统
- 编程语言：Python, Triton
- 课程难度：🌟🌟🌟
- 预计学时：120小时

<!-- 用一两段话介绍这门课程，内容包括但不限于：
    （1）课程覆盖的知识点范围
    （2）与同类课程相比它的优势与特点
    （3）学习这门课程的体验与感受
    （4）自学这门课的注意点（踩过的坑、难度预警等等）
    （5）... ...
-->

本课程专注于设计一个全面的大语言模型(LLM)系统课程，作为设计高效LLM系统的入门介绍。

课程可以更准确地分为三个部分（外加若干 guest lecture）：

1. 基础：现代深度学习与计算表示
   - Modern DL 与计算图（computational graph / framework 基础）
   - Autodiff 与 ML system 架构概览
   - Tensor format、MatMul 深入与硬件加速器（accelerators）

2. 系统与性能优化：从 GPU Kernel 到编译与内存
   - GPUs & CUDA（含基本性能模型）
   - GPU MatMul 与算子编译（operator compilation）
   - Triton 编程、图优化与编译（graph optimization & compilation）
   - Memory（含训练/推理中的内存问题与技巧）
   - Quantization（量化方法与系统落地）

3. LLM系统：训练与推理
   - 并行策略：模型并行、collective communication、intra-/inter-op、自动并行化
   - LLM 基础：Transformer、Attention、MoE
   - LLM 训练优化：FlashAttention 等
   - LLM 推理：continuous batching、paged attention、disaggregated prefill/decoding
   - Scaling law、test-time compute / reasoning，以及 “LLM + X”（RAG / search / multimodality / tool-use / agents 等）

（Guest lectures：ML compiler、LLM pretraining/open science、fast inference、tool use & agents 等，作为补充与扩展。）

CSE234的最大特点在于非常专注于以LLM (LLM System)为核心应用场景，强调真实系统设计中的取舍与工程约束，而非停留在算法或 API 使用层面。课程作业通常需要直接面对性能瓶颈（如内存带宽、通信开销、kernel fusion 等），并通过 Triton 或系统级优化手段加以解决，对理解“为什么某些 LLM 系统设计是现在这个样子”非常有帮助。学习体验整体偏硬核，前期对系统与并行计算背景要求较高，自学时建议提前补齐 CUDA/并行编程与基础系统知识，否则在后半部分（尤其是 LLM 优化与推理相关内容）会明显感到陡峭的学习曲线。但一旦跟上节奏，这门课对从事 LLM Infra / ML Systems / AI Compiler 方向的同学具有很强的长期价值。


## 课程资源

- 课程网站：https://hao-ai-lab.github.io/cse234-w25/
- 课程视频：https://hao-ai-lab.github.io/cse234-w25/
- 课程教材：https://hao-ai-lab.github.io/cse234-w25/resources/
- 课程作业：https://hao-ai-lab.github.io/cse234-w25/assignments/

## 资源汇总

所有课程内容都发布了对应的开源版本，但在线测评和作业参考答案部分尚未开源。