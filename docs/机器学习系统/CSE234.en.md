# CSE234: Data Systems for Machine Learning

## Course Overview

- University: UCSD  
- Prerequisites: Linear Algebra, Deep Learning, Operating Systems, Computer Networks, Distributed Systems  
- Programming Languages: Python, Triton  
- Difficulty: üåüüåüüåü  
- Estimated Workload: ~120 hours  

<!-- Introduce the course in one or two paragraphs, including but not limited to:
     (1) The scope of technical topics covered
     (2) Its advantages and distinguishing features compared to similar courses
     (3) Personal learning experience and impressions
     (4) Caveats and difficulty warnings for self-study
-->

This course focuses on the design of end-to-end large language model (LLM) systems, serving as an introductory yet comprehensive guide to building efficient LLM systems in practice.

The course can be more accurately divided into three main parts (with several additional guest lectures):

1. Foundations: modern deep learning and computational representations  
   - Modern deep learning and computation graphs (framework and system basics)  
   - Automatic differentiation and ML system architecture overview  
   - Tensor formats, in-depth matrix multiplication, and hardware accelerators  

2. Systems and performance optimization: from GPU kernels to compilation and memory  
   - GPUs and CUDA (including basic performance models)  
   - GPU matrix multiplication and operator-level compilation  
   - Triton programming, graph optimization, and compilation  
   - Memory management in training and inference  
   - Quantization techniques and system-level deployment  

3. LLM systems: training and inference  
   - Parallelization strategies: model parallelism, collective communication, intra-/inter-op parallelism, and auto-parallelization  
   - LLM fundamentals: Transformers, Attention, and MoE  
   - LLM training optimizations (e.g., FlashAttention-style techniques)  
   - LLM inference: continuous batching, paged attention, disaggregated prefill/decoding  
   - Scaling laws, test-time compute and reasoning, and ‚ÄúLLM + X‚Äù applications (RAG, search, multimodality, tool use, agents, etc.)

(Guest lectures cover topics such as ML compilers, LLM pretraining and open science, fast inference, and tool use and agents, serving as complementary extensions.)

The defining characteristic of CSE234 is its strong focus on LLM systems as the core application setting. The course emphasizes real-world system design trade-offs and engineering constraints, rather than remaining at the level of algorithms or API usage. Assignments often require students to directly confront performance bottlenecks‚Äîsuch as memory bandwidth limitations, communication overheads, and kernel fusion‚Äîand address them through Triton or system-level optimizations. This makes the course particularly helpful for understanding *why modern LLM systems are designed the way they are*. The learning experience is overall quite intensive: a solid background in systems and parallel computing is important, and for self-study it is strongly recommended to prepare CUDA, parallel programming, and core systems knowledge in advance. Otherwise, the learning curve becomes noticeably steep in the later parts of the course, especially around LLM optimization and inference. That said, once the pace is manageable, the course offers substantial long-term value for those pursuing work in LLM infrastructure, ML systems, or AI compilers.

## Course Resources

- Course Website: https://hao-ai-lab.github.io/cse234-w25/  
- Lecture Videos: https://hao-ai-lab.github.io/cse234-w25/  
- Reading Materials: https://hao-ai-lab.github.io/cse234-w25/resources/  
- Assignments: https://hao-ai-lab.github.io/cse234-w25/assignments/  

## Resource Summary

All course materials are released in open-source form. However, the online grading infrastructure and reference solutions for assignments have not been made public.