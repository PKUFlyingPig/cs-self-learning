# CMU 15-779: Advanced Topics in Machine Learning Systems (LLM Edition)

## 课程简介

- 所属大学：Carnegie Mellon University
- 先修要求：无硬性先修要求；建议具备机器学习入门与深度学习训练经验，熟悉 PyTorch；了解 CUDA/GPU 基础会显著提升学习效率
- 编程语言：Python（系统与算子层面内容可能涉及 CUDA/硬件概念）
- 课程难度：4/5
- 预计学时：80-120 学时

这门课从系统视角系统性回答一个核心问题：一个用高层框架（例如 PyTorch）写出来的模型，是如何被分解为底层 kernel，并在异构硬件加速器（GPU/TPU）与分布式环境中高效执行的。课程覆盖 GPU 编程、ML 编译器、图级优化、分布式训练与自动并行化、LLM Serving 与推理加速等主题，强系统导向，适合希望把“框架层经验”向“算子/编译/硬件/集群执行”打通的人。

从教学组织上，这门课会要求你持续完成课前论文阅读（paper review / reading assignments），并以小组形式完成期末系统类课程项目（proposal、presentation、report），因此自学时建议把它当成一个“按周推进的系统训练营”，而不是只看几份 slide。

## 课程内容

课程内容以 lecture 为主线，主题大致包括：

1. ML 系统基础：以 TensorFlow/PyTorch 为例理解计算图、执行模型与系统抽象
2. GPU 架构与 CUDA 编程：硬件与编程模型、内存与性能优化要点
3. Transformer 与 Attention 案例：FlashAttention 等 IO-aware attention 优化思路
4. 高级 CUDA 编程：warp specialization、mega kernel 等低延迟/高吞吐优化技术
5. ML 编译：Tile-based DSL（Triton 等）、内核自动调优（Ansor 等）、图级优化（TASO/PET 等）、超优化（Mirage）
6. 并行化与分布式训练：ZeRO/FSDP、模型/流水线并行、自动并行化（Alpa 等）
7. LLM 推理与服务：批处理、PagedAttention、RadixAttention、推测解码等
8. 后训练与模型结构：参数高效微调（LoRA/QLoRA）、MoE（架构、kernel、并行化）

## 课程资源

- 课程网站：<https://www.cs.cmu.edu/~zhihaoj2/15-779/>
- 课程安排（含每讲 slide 与阅读列表）：<https://www.cs.cmu.edu/~zhihaoj2/15-779/schedule.html>
- 课程讲义（PDF slides）：<https://www.cs.cmu.edu/~zhihaoj2/15-779/slides/>
- 课程规则与项目要求（Grading、Paper Review、Course Project）：<https://www.cs.cmu.edu/~zhihaoj2/15-779/logistics.html>
- 预备材料（深度学习入门材料汇总）：<https://www.cs.cmu.edu/~zhihaoj2/15-779/materials.html>

