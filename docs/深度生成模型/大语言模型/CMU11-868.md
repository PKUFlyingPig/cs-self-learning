# CMU 11-868: Large Language Model Systems

## 课程简介

- 所属大学：Carnegie Mellon University
- 先修要求：强烈建议已修读 Deep Learning (11785) 或 Advanced NLP (11-611 或 11-711)
- 编程语言：Python
- 课程难度：🌟🌟🌟🌟  
- 预计学时：120 学时  

该课程面向研究生开设，聚焦“从算法到工程”的大语言模型系统构建全过程。课程内容包括但不限于：

1. **GPU 编程与自动微分**：掌握 CUDA kernel 调用、并行编程基础，以及深度学习框架设计原理。  
2. **模型训练与分布式系统**：学习高效的训练算法、通信优化（ZeRO、FlashAttention）、分布式训练框架（DDP、GPipe、Megatron-LM）。  
3. **模型压缩与加速**：量化（GPTQ）、稀疏化（MoE）、编译技术（JAX、Triton）、以及推理时的服务化设计（vLLM、CacheGen）。  
4. **前沿技术与系统实践**：涵盖检索增强生成（RAG）、多模态 LLM、RLHF 系统，以及端到端的在线维护和监控。  

与同类课程相比，本课程的优势在于**紧密结合最新论文与开源实现**（通过 miniTorch 框架动手扩展 CUDA 支持）；**项目驱动**的作业体系（五次编程作业 + 期末大项目）；以及**工业嘉宾讲座**，能让学生近距离了解真实世界中 LLM 工程实践的挑战与解决方案。

**自学建议**：

- 提前配置好支持 CUDA 的开发环境（NVIDIA GPU + CUDA Toolkit + PyTorch）。  
- 复习并行计算和深度学习基础（自动微分、张量运算）。  
- 阅读每次课前指定的论文与幻灯片，跟着作业把 miniTorch 框架从纯 Python 拓展到真实 CUDA 内核。  

该课程要求你对深度学习有一定的预备知识，不适合纯小白入手，可见 [FAQ](https://llmsystem.github.io/llmsystem2024spring/docs/FAQ) 的先修要求。
实验总体来说是有难度的，主要内容如下：

1. Assignment1: 自动微分框架 + CUDA 手写算子 + 基础神经网络构建
2. Assignmant2: GPT2 模型构建
3. Assignment3: 通过手写 CUDA 的 Softmax 和 LayerNorm 算子优化模型训练速度
4. Assignment4: 分布式模型训练，自学的话可能不太好配置环境

## 课程资源

- 课程网站：<https://llmsystem.github.io/llmsystem2025spring/>  
- 课程大纲：<https://llmsystem.github.io/llmsystem2025spring/docs/Syllabus/>  
- 课程作业：<https://llmsystem.github.io/llmsystem2025springhw/>  
- 课程教材：精选论文 + 《Programming Massively Parallel Processors, 4th Ed》 部分章节
