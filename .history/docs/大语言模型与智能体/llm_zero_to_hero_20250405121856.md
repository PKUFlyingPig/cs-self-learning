# Andrej Karpathy LLM Zero to Hero ç³»åˆ—è§†é¢‘

## è¯¾ç¨‹ç®€ä»‹

- æ‰€å±å¤§å­¦ï¼š
- å…ˆä¿®è¦æ±‚ï¼š
- ç¼–ç¨‹è¯­è¨€ï¼špython
- è¯¾ç¨‹éš¾åº¦ï¼šğŸŒŸ
- é¢„è®¡å­¦æ—¶ï¼š50ï½100 å°æ—¶


## è¯¾ç¨‹èµ„æº

- è¯¾ç¨‹ç½‘ç«™ï¼š
- è¯¾ç¨‹è§†é¢‘ï¼š[YouTube](https://www.youtube.com/@AndrejKarpathy)
- è¯¾ç¨‹æ•™æï¼š
- è¯¾ç¨‹ä½œä¸šï¼š

## èµ„æºæ±‡æ€»

LLM general guidance

-[Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g)

### è§†é¢‘ä¸»é¢˜ï¼š[1hr Talk] Intro to Large Language Models


#### æ ¸å¿ƒå†…å®¹

##### 1. å¤§å‹è¯­è¨€æ¨¡å‹çš„å®šä¹‰ä¸ç»„æˆ
- **å®šä¹‰**ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯ä¸€ç§åŸºäºç¥ç»ç½‘ç»œçš„æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹æ–‡æœ¬åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚
- **ç»„æˆ**ï¼šä»¥Llama 270bæ¨¡å‹ä¸ºä¾‹ï¼Œæ¨¡å‹ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š
  - **å‚æ•°æ–‡ä»¶**ï¼šåŒ…å«æ¨¡å‹çš„æƒé‡ï¼Œæ–‡ä»¶å¤§å°çº¦ä¸º140GBï¼ˆ700äº¿å‚æ•°ï¼Œæ¯ä¸ªå‚æ•°å ç”¨2å­—èŠ‚ï¼‰ã€‚
  - **è¿è¡Œä»£ç **ï¼šç”¨äºåŠ è½½å’Œè¿è¡Œå‚æ•°æ–‡ä»¶ï¼Œå¯ä½¿ç”¨Cè¯­è¨€å®ç°ï¼Œä»£ç é‡çº¦500è¡Œã€‚

##### 2. æ¨¡å‹è®­ç»ƒä¸æ¨ç†
- **æ¨¡å‹è®­ç»ƒ**ï¼š
  - **æ•°æ®é‡**ï¼šè®­ç»ƒæ•°æ®é‡å·¨å¤§ï¼Œçº¦10TBæ–‡æœ¬æ•°æ®ï¼Œé€šå¸¸æ¥è‡ªäº’è”ç½‘çˆ¬å–ã€‚
  - **è®¡ç®—èµ„æº**ï¼šéœ€è¦çº¦6000ä¸ªGPUï¼Œè®­ç»ƒæ—¶é—´çº¦12å¤©ï¼Œæˆæœ¬çº¦200ä¸‡ç¾å…ƒã€‚
  - **è®­ç»ƒè¿‡ç¨‹**ï¼šå°†å¤§é‡æ–‡æœ¬æ•°æ®å‹ç¼©æˆæ¨¡å‹å‚æ•°ï¼Œç±»ä¼¼äºâ€œå‹ç¼©æ–‡ä»¶â€ï¼Œä½†ä¸ºæœ‰æŸå‹ç¼©ã€‚
- **æ¨¡å‹æ¨ç†**ï¼š
  - **ç®€å•æ€§**ï¼šåœ¨MacBookä¸Šå³å¯è¿è¡Œï¼Œæ— éœ€äº’è”ç½‘è¿æ¥ã€‚
  - **é€Ÿåº¦**ï¼š700äº¿å‚æ•°æ¨¡å‹è¿è¡Œé€Ÿåº¦è¾ƒæ…¢ï¼Œä½†è¾ƒå°æ¨¡å‹ï¼ˆå¦‚7äº¿å‚æ•°ï¼‰è¿è¡Œé€Ÿåº¦è¾ƒå¿«ã€‚

##### 3. æ¨¡å‹çš„åº”ç”¨ä¸èƒ½åŠ›
- **æ–‡æœ¬ç”Ÿæˆ**ï¼šæ¨¡å‹å¯ä»¥æ ¹æ®è¾“å…¥çš„æ–‡æœ¬ç”Ÿæˆåç»­å†…å®¹ï¼Œä¾‹å¦‚å†™è¯—ã€ç”Ÿæˆæ–‡ç« ç­‰ã€‚
- **çŸ¥è¯†è¡¨ç¤º**ï¼šæ¨¡å‹é€šè¿‡é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œå­¦ä¹ äº†å¤§é‡å…³äºä¸–ç•Œçš„çŸ¥è¯†ï¼Œä½†è¿™ç§çŸ¥è¯†æ˜¯â€œå‹ç¼©â€å’Œâ€œæœ‰æŸâ€çš„ã€‚
- **æ¨¡å‹çš„å±€é™æ€§**ï¼šå°½ç®¡æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆçœ‹ä¼¼åˆç†çš„æ–‡æœ¬ï¼Œä½†å…¶è¾“å‡ºæœ‰æ—¶å¯èƒ½æ˜¯é”™è¯¯çš„æˆ–â€œå¹»è§‰â€ï¼ˆhallucinationï¼‰ã€‚

##### 4. æ¨¡å‹çš„è®­ç»ƒé˜¶æ®µ
- **é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰**ï¼š
  - ä½¿ç”¨å¤§é‡äº’è”ç½‘æ–‡æ¡£è¿›è¡Œè®­ç»ƒï¼Œç›®æ ‡æ˜¯å­¦ä¹ è¯­è¨€çš„æ¨¡å¼å’ŒçŸ¥è¯†ã€‚
- **å¾®è°ƒï¼ˆFine-tuningï¼‰**ï¼š
  - ä½¿ç”¨äººå·¥æ ‡æ³¨çš„é—®ç­”æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿä»¥â€œåŠ©æ‰‹â€çš„å½¢å¼å›ç­”é—®é¢˜ã€‚
  - å¾®è°ƒé˜¶æ®µæ›´æ³¨é‡è´¨é‡è€Œéæ•°é‡ï¼Œæ•°æ®é‡ç›¸å¯¹è¾ƒå°‘ä½†è´¨é‡æ›´é«˜ã€‚

##### 5. æ¨¡å‹çš„ä¼˜åŒ–ä¸æ”¹è¿›
- **å¤šæ¨¡æ€èƒ½åŠ›**ï¼šæ¨¡å‹ä¸ä»…èƒ½å¤Ÿç”Ÿæˆæ–‡æœ¬ï¼Œè¿˜èƒ½ç”Ÿæˆå›¾åƒã€å¤„ç†éŸ³é¢‘ç­‰ã€‚
- **å·¥å…·ä½¿ç”¨**ï¼šæ¨¡å‹èƒ½å¤Ÿåˆ©ç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚æœç´¢å¼•æ“ã€è®¡ç®—å™¨ç­‰ï¼‰æ¥å®Œæˆä»»åŠ¡ï¼Œç±»ä¼¼äºäººç±»ä½¿ç”¨å·¥å…·è§£å†³é—®é¢˜ã€‚
- **ç³»ç»Ÿä¸€ä¸ç³»ç»ŸäºŒæ€ç»´**ï¼šå½“å‰æ¨¡å‹ä¸»è¦ä¾èµ–å¿«é€Ÿã€ç›´è§‰çš„ç³»ç»Ÿä¸€æ€ç»´ï¼Œæœªæ¥å¯èƒ½å‘å±•å‡ºæ›´å¤æ‚çš„ã€ç±»ä¼¼äººç±»çš„ç³»ç»ŸäºŒæ€ç»´ã€‚

##### 6. æ¨¡å‹çš„å®‰å…¨æ€§ä¸æŒ‘æˆ˜
- **æ”»å‡»ç±»å‹**ï¼š
  - **è¶Šç‹±æ”»å‡»ï¼ˆJailbreak Attacksï¼‰**ï¼šé€šè¿‡ç‰¹å®šçš„æç¤ºæˆ–è§’è‰²æ‰®æ¼”ï¼Œä½¿æ¨¡å‹ç»•è¿‡å®‰å…¨é™åˆ¶ï¼Œè¾“å‡ºæœ‰å®³å†…å®¹ã€‚
  - **æç¤ºæ³¨å…¥æ”»å‡»ï¼ˆPrompt Injection Attacksï¼‰**ï¼šé€šè¿‡åœ¨è¾“å…¥ä¸­åµŒå…¥éšè—çš„æŒ‡ä»¤ï¼ŒåŠ«æŒæ¨¡å‹çš„è¾“å‡ºã€‚
  - **æ•°æ®æŠ•æ¯’æ”»å‡»ï¼ˆData Poisoning Attacksï¼‰**ï¼šé€šè¿‡åœ¨è®­ç»ƒæ•°æ®ä¸­æ’å…¥ç‰¹å®šçš„è§¦å‘è¯ï¼Œä½¿æ¨¡å‹åœ¨é‡åˆ°è¿™äº›è¯æ—¶äº§ç”Ÿé”™è¯¯çš„è¾“å‡ºã€‚

#### ç»“è®º
- å¤§å‹è¯­è¨€æ¨¡å‹å…·æœ‰å¼ºå¤§çš„æ–‡æœ¬ç”Ÿæˆå’ŒçŸ¥è¯†è¡¨ç¤ºèƒ½åŠ›ï¼Œä½†å…¶è®­ç»ƒå’Œè¿è¡Œéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œæ•°æ®ã€‚
- æ¨¡å‹çš„å¾®è°ƒé˜¶æ®µèƒ½å¤Ÿä½¿å…¶æ›´å¥½åœ°é€‚åº”ç‰¹å®šçš„ä»»åŠ¡å’Œæ ¼å¼ã€‚
- æ¨¡å‹çš„å¤šæ¨¡æ€èƒ½åŠ›å’Œå·¥å…·ä½¿ç”¨æ˜¯æœªæ¥å‘å±•çš„å…³é”®æ–¹å‘ã€‚
- æ¨¡å‹çš„å®‰å…¨æ€§æ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ï¼Œéœ€è¦ä¸æ–­ç ”ç©¶å’Œæ”¹è¿›ã€‚


[Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI)

2. **LLMsçš„è®­ç»ƒè¿‡ç¨‹**
   - **é¢„è®­ç»ƒé˜¶æ®µï¼ˆPre-training Stageï¼‰**ï¼š
     - **æ•°æ®æ”¶é›†**ï¼šä»äº’è”ç½‘ä¸Šä¸‹è½½å’Œå¤„ç†å¤§é‡æ–‡æœ¬æ•°æ®ï¼Œæ„å»ºé«˜è´¨é‡çš„æ–‡æœ¬æ•°æ®é›†ã€‚ä¾‹å¦‚ï¼ŒHugging Faceçš„Fine Webæ•°æ®é›†åŒ…å«çº¦44TBçš„æ•°æ®ï¼Œç»è¿‡å¤šé˜¶æ®µå¤„ç†ï¼Œæœ€ç»ˆå¾—åˆ°çº¦15ä¸‡äº¿ä¸ªtokençš„æ•°æ®é›†ã€‚
     - **æ–‡æœ¬æå–ä¸è¿‡æ»¤**ï¼šé€šè¿‡URLè¿‡æ»¤ã€æ–‡æœ¬æå–ã€è¯­è¨€è¿‡æ»¤ç­‰æ­¥éª¤ï¼Œå»é™¤ä½è´¨é‡æˆ–ä¸ç›¸å…³å†…å®¹ï¼Œä¿ç•™é«˜è´¨é‡ã€å¤šæ ·åŒ–çš„æ–‡æœ¬ã€‚
     - **TokenåŒ–**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„tokenåºåˆ—ã€‚ä½¿ç”¨Byte Pair Encodingï¼ˆBPEï¼‰ç®—æ³•å‡å°‘åºåˆ—é•¿åº¦ï¼Œå¢åŠ è¯æ±‡è¡¨å¤§å°ã€‚ä¾‹å¦‚ï¼ŒGPT-4ä½¿ç”¨çº¦100,000ä¸ªç¬¦å·ã€‚
   - **ç¥ç»ç½‘ç»œè®­ç»ƒ**ï¼š
     - **è¾“å…¥ä¸è¾“å‡º**ï¼šæ¨¡å‹çš„è¾“å…¥æ˜¯tokenåºåˆ—ï¼Œè¾“å‡ºæ˜¯å¯¹ä¸‹ä¸€ä¸ªtokençš„é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒã€‚
     - **è®­ç»ƒè¿‡ç¨‹**ï¼šé€šè¿‡å¤§é‡æ•°æ®è®­ç»ƒç¥ç»ç½‘ç»œï¼Œä½¿å…¶é¢„æµ‹çš„tokenåºåˆ—ä¸è®­ç»ƒæ•°æ®ä¸­çš„å®é™…åºåˆ—ä¸€è‡´ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹çš„å‚æ•°ä¼šä¸æ–­è°ƒæ•´ï¼Œä»¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚
     - **ç¥ç»ç½‘ç»œå†…éƒ¨ç»“æ„**ï¼šä»‹ç»äº†ç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»“æ„ï¼Œå¦‚Transformeræ¶æ„ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡çŸ©é˜µè¿ç®—å’Œæ¿€æ´»å‡½æ•°è¿›è¡Œè®¡ç®—ã€‚

3. **LLMsçš„æ¨ç†è¿‡ç¨‹ï¼ˆInferenceï¼‰**
   - **ç”Ÿæˆæ–‡æœ¬**ï¼šåœ¨æ¨ç†é˜¶æ®µï¼Œæ¨¡å‹æ ¹æ®è¾“å…¥çš„tokenåºåˆ—ç”Ÿæˆæ–°çš„æ–‡æœ¬ã€‚é€šè¿‡é‡‡æ ·ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ¨¡å‹å¯ä»¥ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬ã€‚
   - **éšæœºæ€§ä¸å¤šæ ·æ€§**ï¼šç”±äºæ¨¡å‹çš„è¾“å‡ºæ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œæ¯æ¬¡ç”Ÿæˆçš„ç»“æœå¯èƒ½ä¸åŒï¼Œè¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿäº§ç”Ÿå¤šæ ·åŒ–çš„æ–‡æœ¬ã€‚

4. **LLMsçš„å®é™…åº”ç”¨ä¸å±€é™æ€§**
   - **ä¼˜åŠ¿**ï¼šLLMsåœ¨å¤„ç†è‡ªç„¶è¯­è¨€ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬ï¼Œå¦‚å†™ä½œã€ç¿»è¯‘ã€é—®ç­”ç­‰ã€‚
   - **å±€é™æ€§**ï¼š
     - **å¹»è§‰ï¼ˆHallucinationsï¼‰**ï¼šæ¨¡å‹å¯èƒ½ä¼šç”Ÿæˆè™šå‡æˆ–ä¸å‡†ç¡®çš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå½“æ¨¡å‹è¢«é—®åŠä¸€ä¸ªä¸å­˜åœ¨çš„äººç‰©æ—¶ï¼Œå®ƒå¯èƒ½ä¼šç¼–é€ ä¸€ä¸ªçœ‹ä¼¼åˆç†çš„å›ç­”ã€‚
     - **è®¡ç®—èµ„æºéœ€æ±‚**ï¼šè®­ç»ƒå’Œè¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œé€šå¸¸éœ€è¦ä½¿ç”¨é«˜æ€§èƒ½çš„GPUé›†ç¾¤ã€‚
     - **æ•°æ®ä¾èµ–æ€§**ï¼šæ¨¡å‹çš„æ€§èƒ½ä¾èµ–äºè®­ç»ƒæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§ã€‚å¦‚æœè®­ç»ƒæ•°æ®å­˜åœ¨åå·®æˆ–ä¸å‡†ç¡®ï¼Œæ¨¡å‹çš„è¾“å‡ºä¹Ÿå¯èƒ½å—åˆ°å½±å“ã€‚

5. **LLMsçš„å¿ƒç†å­¦ç‰¹æ€§ï¼ˆLLM Psychologyï¼‰**
   - **è‡ªæˆ‘è®¤çŸ¥ï¼ˆSense of Selfï¼‰**ï¼šæ¨¡å‹æ²¡æœ‰æŒä¹…çš„è‡ªæˆ‘è®¤çŸ¥ï¼Œæ¯æ¬¡å¯¹è¯éƒ½æ˜¯ä»å¤´å¼€å§‹çš„ã€‚
   - **é—®é¢˜è§£å†³èƒ½åŠ›**ï¼šæ¨¡å‹åœ¨è§£å†³å¤æ‚é—®é¢˜æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ç®€å•é—®é¢˜ä¸Šå¯èƒ½ä¼šå‡ºç°é”™è¯¯ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹å¯èƒ½åœ¨è§£å†³æ•°å­¦å¥¥æ—åŒ¹å…‹é—®é¢˜æ—¶è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨æ¯”è¾ƒä¸¤ä¸ªæ•°å­—å¤§å°æ—¶å´å¯èƒ½å‡ºç°é”™è¯¯ã€‚
   - **å·¥å…·ä½¿ç”¨ï¼ˆTool Useï¼‰**ï¼šæ¨¡å‹å¯ä»¥é€šè¿‡è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚æœç´¢å¼•æ“ã€ä»£ç è§£é‡Šå™¨ï¼‰æ¥æé«˜å…¶è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œå½“æ¨¡å‹ä¸ç¡®å®šæŸä¸ªé—®é¢˜çš„ç­”æ¡ˆæ—¶ï¼Œå®ƒå¯ä»¥è°ƒç”¨æœç´¢å¼•æ“è·å–æ›´å¤šä¿¡æ¯ã€‚

6. **å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰**
   - å¼ºåŒ–å­¦ä¹ æ˜¯LLMsè®­ç»ƒçš„ç¬¬ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼Œæ—¨åœ¨é€šè¿‡å®è·µé—®é¢˜æ¥æé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚
   - åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæ¨¡å‹ä¼šå°è¯•å¤šç§è§£å†³æ–¹æ¡ˆï¼Œå¹¶æ ¹æ®æœ€ç»ˆç­”æ¡ˆçš„æ­£ç¡®æ€§æ¥è°ƒæ•´å…¶è¡Œä¸ºã€‚
   - é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼Œæ¨¡å‹å¯ä»¥å‘ç°æ›´æœ‰æ•ˆçš„è§£å†³é—®é¢˜çš„ç­–ç•¥ï¼Œæé«˜å…¶æ¨ç†èƒ½åŠ›ã€‚

[How I use LLMs](https://www.youtube.com/watch?v=EWvNQjAaOHw)

ä»¥ä¸‹æ˜¯å…³äºè§†é¢‘ã€ŠHow I use LLMs - YouTubeã€‹çš„è¯¦å°½ç¬”è®°ï¼š
## è§†é¢‘å†…å®¹æ¦‚è¿°

### 1. LLMsçš„åº”ç”¨èƒŒæ™¯
- **å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç®€ä»‹**ï¼šè§†é¢‘å¼€å¤´æåˆ°ï¼ŒLLMsï¼ˆå¦‚ChatGPTï¼‰æ˜¯åŸºäºæ·±åº¦å­¦ä¹ æŠ€æœ¯è®­ç»ƒçš„æ¨¡å‹ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬ã€‚è¿™äº›æ¨¡å‹é€šè¿‡å¤§é‡çš„äº’è”ç½‘æ–‡æœ¬æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»è€Œè·å¾—å¹¿æ³›çš„çŸ¥è¯†å’Œè¯­è¨€èƒ½åŠ›ã€‚
- **åº”ç”¨åœºæ™¯**ï¼šLLMså¯ä»¥åº”ç”¨äºå¤šç§åœºæ™¯ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºå†™ä½œã€ç¼–ç¨‹è¾…åŠ©ã€æ•°æ®åˆ†æã€çŸ¥è¯†é—®ç­”ç­‰ã€‚è§†é¢‘ä¸­æåˆ°ï¼Œéšç€æŠ€æœ¯çš„å‘å±•ï¼ŒLLMsçš„åº”ç”¨èŒƒå›´è¿˜åœ¨ä¸æ–­æ‰©å¤§ã€‚

### 2. ChatGPTçš„ä½¿ç”¨ä½“éªŒ
- **ChatGPTçš„åŸºæœ¬åŠŸèƒ½**ï¼šè§†é¢‘ä¸­å±•ç¤ºäº†ChatGPTçš„åŸºæœ¬äº¤äº’æ–¹å¼ï¼Œå³ç”¨æˆ·è¾“å…¥æ–‡æœ¬é—®é¢˜ï¼Œæ¨¡å‹ç”Ÿæˆç›¸åº”çš„æ–‡æœ¬å›ç­”ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥è¯¢é—®å…³äºå’–å•¡å› å«é‡çš„é—®é¢˜ï¼Œæ¨¡å‹èƒ½å¤Ÿç»™å‡ºå¤§è‡´çš„ç­”æ¡ˆã€‚
- **æ¨¡å‹çš„å±€é™æ€§**ï¼šå°½ç®¡ChatGPTåŠŸèƒ½å¼ºå¤§ï¼Œä½†å®ƒä¹Ÿæœ‰å±€é™æ€§ã€‚è§†é¢‘æåˆ°ï¼Œæ¨¡å‹çš„çŸ¥è¯†æˆªæ­¢æ—¥æœŸæ˜¯å…¶é¢„è®­ç»ƒå®Œæˆçš„æ—¶é—´ï¼Œè¿™æ„å‘³ç€å®ƒå¯¹è¿‘æœŸå‘ç”Ÿçš„äº‹ä»¶æˆ–æœ€æ–°çš„ä¿¡æ¯å¯èƒ½ä¸äº†è§£ã€‚æ­¤å¤–ï¼Œæ¨¡å‹çš„å›ç­”æ˜¯åŸºäºæ¦‚ç‡å’Œç»Ÿè®¡çš„ï¼Œå› æ­¤å¯èƒ½ä¸å®Œå…¨å‡†ç¡®ï¼Œç”¨æˆ·éœ€è¦è‡ªè¡ŒéªŒè¯å…³é”®ä¿¡æ¯ã€‚

### 3. ä¸åŒLLMsçš„æ¯”è¾ƒ
- **å…¶ä»–LLMsçš„ä»‹ç»**ï¼šé™¤äº†ChatGPTï¼Œè§†é¢‘è¿˜æåˆ°äº†å…¶ä»–ä¸€äº›LLMsï¼Œå¦‚Googleçš„Geminiã€Metaçš„Co-pilotã€Anthropicçš„Claudeç­‰ã€‚è¿™äº›æ¨¡å‹å„æœ‰ç‰¹ç‚¹ï¼Œæœ‰çš„åœ¨ç‰¹å®šé¢†åŸŸè¡¨ç°æ›´ä¼˜ï¼Œæœ‰çš„åˆ™æä¾›äº†ä¸åŒçš„ç”¨æˆ·ä½“éªŒã€‚
- **å¦‚ä½•é€‰æ‹©LLMs**ï¼šè§†é¢‘å»ºè®®ç”¨æˆ·æ ¹æ®è‡ªå·±çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„LLMsã€‚ä¸åŒçš„LLMsåœ¨åŠŸèƒ½ã€æ€§èƒ½å’Œä»·æ ¼ä¸Šå­˜åœ¨å·®å¼‚ï¼Œç”¨æˆ·éœ€è¦æ ¹æ®è‡ªå·±çš„é¢„ç®—å’Œä½¿ç”¨åœºæ™¯æ¥å†³å®šã€‚

### 4. LLMsçš„é«˜çº§åŠŸèƒ½
- **æ€è€ƒæ¨¡å‹ï¼ˆThinking Modelsï¼‰**ï¼šè§†é¢‘ä¸­æåˆ°ï¼Œä¸€äº›LLMsç»è¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œèƒ½å¤Ÿè¿›è¡Œæ›´æ·±å…¥çš„æ€è€ƒå’Œæ¨ç†ã€‚è¿™äº›æ¨¡å‹åœ¨è§£å†³å¤æ‚çš„æ•°å­¦é—®é¢˜ã€ç¼–ç¨‹é—®é¢˜ç­‰æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œä½†å¯èƒ½éœ€è¦æ›´é•¿çš„æ—¶é—´æ¥ç”Ÿæˆå›ç­”ã€‚
- **å·¥å…·ä½¿ç”¨ï¼ˆTool Useï¼‰**ï¼šLLMså¯ä»¥ä¸å„ç§å·¥å…·é›†æˆï¼Œä¾‹å¦‚äº’è”ç½‘æœç´¢ã€Pythonè§£é‡Šå™¨ç­‰ã€‚é€šè¿‡è¿™äº›å·¥å…·ï¼Œæ¨¡å‹èƒ½å¤Ÿè·å–æœ€æ–°çš„ä¿¡æ¯æˆ–æ‰§è¡Œå¤æ‚çš„è®¡ç®—ä»»åŠ¡ã€‚

### 5. LLMsçš„å¤šæ¨¡æ€äº¤äº’
- **è¯­éŸ³äº¤äº’**ï¼šè§†é¢‘å±•ç¤ºäº†å¦‚ä½•é€šè¿‡è¯­éŸ³ä¸LLMsè¿›è¡Œäº¤äº’ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡è¯­éŸ³è¾“å…¥é—®é¢˜ï¼Œæ¨¡å‹å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬å¹¶ç”Ÿæˆå›ç­”ã€‚æ­¤å¤–ï¼Œä¸€äº›LLMsè¿˜æ”¯æŒå°†æ–‡æœ¬å›ç­”è½¬æ¢ä¸ºè¯­éŸ³è¾“å‡ºã€‚
- **å›¾åƒå’Œè§†é¢‘å¤„ç†**ï¼šLLMsä¸ä»…å¯ä»¥å¤„ç†æ–‡æœ¬ï¼Œè¿˜å¯ä»¥å¤„ç†å›¾åƒå’Œè§†é¢‘ã€‚ç”¨æˆ·å¯ä»¥ä¸Šä¼ å›¾åƒæˆ–è§†é¢‘ï¼Œæ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å…¶ä¸­çš„å†…å®¹å¹¶ç”Ÿæˆç›¸å…³çš„å›ç­”ã€‚

### 6. LLMsçš„å®ç”¨åŠŸèƒ½
- **è®°å¿†åŠŸèƒ½ï¼ˆMemoryï¼‰**ï¼šChatGPTå…·æœ‰è®°å¿†åŠŸèƒ½ï¼Œèƒ½å¤Ÿè®°ä½ç”¨æˆ·åœ¨ä¸åŒå¯¹è¯ä¸­çš„ä¿¡æ¯ï¼Œä»è€Œæä¾›æ›´ä¸ªæ€§åŒ–çš„å›ç­”ã€‚
- **è‡ªå®šä¹‰æŒ‡ä»¤ï¼ˆCustom Instructionsï¼‰**ï¼šç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚å¯¹LLMsè¿›è¡Œè‡ªå®šä¹‰ï¼Œä¾‹å¦‚è®¾ç½®æ¨¡å‹çš„å›ç­”é£æ ¼ã€è¯­è¨€åå¥½ç­‰ã€‚
- **å¤šè¯­è¨€æ”¯æŒ**ï¼šLLMsæ”¯æŒå¤šç§è¯­è¨€ï¼Œç”¨æˆ·å¯ä»¥ä½¿ç”¨ä¸åŒçš„è¯­è¨€ä¸æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚


Neural Networks: Zero to Hero
A course by Andrej Karpathy on building neural networks, from scratch, in code.
We start with the basics of backpropagation and build up to modern deep neural networks, like GPT. In my opinion language models are an excellent place to learn deep learning, even if your intention is to eventually go to other areas like computer vision because most of what you learn will be immediately transferable. This is why we dive into and focus on languade models.
Prerequisites: solid programming (Python), intro-level math (e.g. derivative, gaussian).

Learning is easier with others, come say hi in our Discord channel:

Syllabus
2h25m The spelled-out intro to neural networks and backpropagation: building micrograd
This is the most step-by-step spelled-out explanation of backpropagation and training of neural networks. It only assumes basic knowledge of Python and a vague recollection of calculus from high school.
1h57m The spelled-out intro to language modeling: building makemore
We implement a bigram character-level language model, which we will further complexify in followup videos into a modern Transformer language model, like GPT. In this video, the focus is on (1) introducing torch.Tensor and its subtleties and use in efficiently evaluating neural networks and (2) the overall framework of language modeling that includes model training, sampling, and the evaluation of a loss (e.g. the negative log likelihood for classification).
1h15m Building makemore Part 2: MLP
We implement a multilayer perceptron (MLP) character-level language model. In this video we also introduce many basics of machine learning (e.g. model training, learning rate tuning, hyperparameters, evaluation, train/dev/test splits, under/overfitting, etc.).
1h55m Building makemore Part 3: Activations & Gradients, BatchNorm
We dive into some of the internals of MLPs with multiple layers and scrutinize the statistics of the forward pass activations, backward pass gradients, and some of the pitfalls when they are improperly scaled. We also look at the typical diagnostic tools and visualizations you'd want to use to understand the health of your deep network. We learn why training deep neural nets can be fragile and introduce the first modern innovation that made doing so much easier: Batch Normalization. Residual connections and the Adam optimizer remain notable todos for later video.
1h55m Building makemore Part 4: Becoming a Backprop Ninja
We take the 2-layer MLP (with BatchNorm) from the previous video and backpropagate through it manually without using PyTorch autograd's loss.backward(): through the cross entropy loss, 2nd linear layer, tanh, batchnorm, 1st linear layer, and the embedding table. Along the way, we get a strong intuitive understanding about how gradients flow backwards through the compute graph and on the level of efficient Tensors, not just individual scalars like in micrograd. This helps build competence and intuition around how neural nets are optimized and sets you up to more confidently innovate on and debug modern neural networks.
56m Building makemore Part 5: Building a WaveNet
We take the 2-layer MLP from previous video and make it deeper with a tree-like structure, arriving at a convolutional neural network architecture similar to the WaveNet (2016) from DeepMind. In the WaveNet paper, the same hierarchical architecture is implemented more efficiently using causal dilated convolutions (not yet covered). Along the way we get a better sense of torch.nn and what it is and how it works under the hood, and what a typical deep learning development process looks like (a lot of reading of documentation, keeping track of multidimensional tensor shapes, moving between jupyter notebooks and repository code, ...).
1h56m Let's build GPT: from scratch, in code, spelled out.
We build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video.
2h13m Let's build the GPT Tokenizer
The Tokenizer is a necessary and pervasive component of Large Language Models (LLMs), where it translates between strings and tokens (text chunks). Tokenizers are a completely separate stage of the LLM pipeline: they have their own training sets, training algorithms (Byte Pair Encoding), and after training implement two fundamental functions: encode() from strings to tokens, and decode() back from tokens to strings. In this lecture we build from scratch the Tokenizer used in the GPT series from OpenAI. In the process, we will see that a lot of weird behaviors and problems of LLMs actually trace back to tokenization. We'll go through a number of these issues, discuss why tokenization is at fault, and why someone out there ideally finds a way to delete this stage entirely.

ä»¥ä¸‹æ˜¯å…³äºè§†é¢‘ã€ŠLet's build GPT: from scratch, in code, spelled out.ã€‹çš„è¯¦å°½ç¬”è®°ã€‚
[text](https://www.youtube.com/watch?v=kCc8FmEb1nY)
ä¸»è¦ä»‹ç»äº†å¦‚ä½•ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªç±»ä¼¼GPTï¼ˆGenerative Pre-trained Transformerï¼‰çš„è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬å…¶èƒŒåçš„åŸç†ã€å®ç°æ–¹æ³•å’Œè®­ç»ƒè¿‡ç¨‹ã€‚



[12] https://github.com/karpathy/llm.c
[13] https://github.com/karpathy/nanoGPT