# Andrej Karpathy LLM Zero to Hero ç³»åˆ—è§†é¢‘

## è¯¾ç¨‹ç®€ä»‹

- æ‰€å±å¤§å­¦ï¼š
- å…ˆä¿®è¦æ±‚ï¼š
- ç¼–ç¨‹è¯­è¨€ï¼špython
- è¯¾ç¨‹éš¾åº¦ï¼šğŸŒŸ
- é¢„è®¡å­¦æ—¶ï¼š50ï½100 å°æ—¶


## è¯¾ç¨‹èµ„æº

- è¯¾ç¨‹ç½‘ç«™ï¼š
- è¯¾ç¨‹è§†é¢‘ï¼š[Youtu](https://www.youtube.com/@AndrejKarpathy)
- è¯¾ç¨‹æ•™æï¼š
- è¯¾ç¨‹ä½œä¸šï¼š

## èµ„æºæ±‡æ€»


## è¯¾ç¨‹æ ¸å¿ƒæ¶æ„

### 1. ç¥ç»ç½‘ç»œåŸºç¡€ä¸å®ç°è·¯å¾„

è¯¥ç³»åˆ—é‡‡ç”¨**"è‡ªåº•å‘ä¸Š"**çš„æ•™å­¦èŒƒå¼ï¼Œä»¥Pythonè¯­è¨€ä¸ºå·¥å…·é“¾ï¼Œæ„å»ºä»åŸºç¡€æ•°å­¦è¿ç®—åˆ°å®Œæ•´GPTæ¨¡å‹çš„å®Œæ•´çŸ¥è¯†ä½“ç³»ã€‚è¯¾ç¨‹æŠ€æœ¯æ ˆæ¼”è¿›è·¯å¾„å¯æ¦‚æ‹¬ä¸ºï¼š

#### å¾®æ¢¯åº¦å¼•æ“ â†’ å¤šå±‚æ„ŸçŸ¥æœº â†’ å·ç§¯ç½‘ç»œ â†’ Transformer â†’ GPTæ¶æ„ â†’ åˆ†è¯ç³»ç»Ÿ

é€šè¿‡microgradæ¨¡å—å®ç°è‡ªåŠ¨å¾®åˆ†å¼•æ“ï¼ˆçº¦200è¡ŒPythonä»£ç ï¼‰ï¼Œå»ºç«‹å¯¹åå‘ä¼ æ’­ç®—æ³•çš„ç›´è§‚ç†è§£[1][3]ã€‚åœ¨makemoreç³»åˆ—å®éªŒä¸­ï¼Œé€æ­¥å¼•å…¥æ‰¹æ¬¡å½’ä¸€åŒ–ã€æ®‹å·®è¿æ¥ç­‰ç°ä»£æ·±åº¦å­¦ä¹ ç»„ä»¶ï¼Œæœ€ç»ˆåœ¨nanoGPTé¡¹ç›®ä¸­å®ç°å®Œæ•´çš„Transformeræ¶æ„[2][3]ã€‚è¿™ç§æ¸è¿›å¼æ•™å­¦æ³•ä½¿å¾—å¤æ‚æ¦‚å¿µå¦‚ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰å’Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Attentionï¼‰çš„æ•°å­¦åŸç†å¾—ä»¥æ¸…æ™°å±•ç°ã€‚

### 2. Transformeræ¶æ„æŠ€æœ¯è§£æ

åœ¨[Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)ä¸“é¢˜è®²åº§ä¸­ï¼ŒKarpathyé€è¡Œç¼–ç æ¼”ç¤ºäº†Transformeræ ¸å¿ƒç»„ä»¶ï¼š

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.proj = nn.Linear(n_embd, n_embd)  # æŠ•å½±å±‚ç¡®ä¿ç»´åº¦åŒ¹é…
    
    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.proj(out)
        return out
```

è¯¥å®ç°æ­ç¤ºäº†æ³¨æ„åŠ›å¤´çš„å¹¶è¡Œè®¡ç®—æœºåˆ¶ï¼Œæ¯ä¸ªå¤´ç‹¬ç«‹å¤„ç†è¾“å…¥å‘é‡çš„å­ç©ºé—´ç‰¹å¾ï¼Œæœ€åé€šè¿‡çº¿æ€§æŠ•å½±æ•´åˆä¿¡æ¯[2]ã€‚è§†é¢‘ä¸­ç‰¹åˆ«å¼ºè°ƒäº†LayerNormä¸BatchNormçš„å·®å¼‚ï¼šå‰è€…åœ¨ç‰¹å¾ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œé€‚ç”¨äºåºåˆ—æ•°æ®ï¼›åè€…åœ¨æ‰¹æ¬¡ç»´åº¦å½’ä¸€åŒ–ï¼Œæ›´é€‚ç”¨äºå›ºå®šé•¿åº¦è¾“å…¥ã€‚

### 3. åˆ†è¯ç³»ç»Ÿå®ç°ç»†èŠ‚

è¯¾ç¨‹ä¸­åŒ…å«å®Œæ•´çš„å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰å®ç°æ•™ç¨‹ï¼Œå…³é”®ç®—æ³•æ­¥éª¤åŒ…æ‹¬ï¼š

1. ç»Ÿè®¡æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹é¢‘ç‡
2. åˆå¹¶æœ€é«˜é¢‘å­—ç¬¦å¯¹å½¢æˆæ–°è¯å…ƒ
3. è¿­ä»£æ‰§è¡Œç›´è‡³è¾¾åˆ°é¢„è®¾è¯æ±‡é‡

å®éªŒæ˜¾ç¤ºï¼Œåœ¨èå£«æ¯”äºšæ–‡æœ¬æ•°æ®ä¸Šï¼Œç»è¿‡10,000æ¬¡åˆå¹¶æ“ä½œåï¼Œåˆ†è¯é”™è¯¯ç‡ä»åˆå§‹çš„23%ä¸‹é™è‡³4.5%[1]ã€‚è¿™ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•æœ‰æ•ˆå¹³è¡¡äº†è¯è¡¨è§„æ¨¡ä¸è¯­ä¹‰ç²’åº¦ï¼Œä¸ºåç»­è¯­è¨€æ¨¡å‹è®­ç»ƒå¥ å®šåŸºç¡€ã€‚

### 4. nanoGPTå·¥ç¨‹å®è·µ

nanoGPTé¡¹ç›®å±•ç°äº†å·¥ä¸šçº§è¯­è¨€æ¨¡å‹çš„ç®€åŒ–å®ç°ï¼Œå…¶è¶…å‚æ•°é…ç½®ç­–ç•¥å…·æœ‰é‡è¦å‚è€ƒä»·å€¼ï¼š

| å‚æ•°            | å…¸å‹å€¼     | ä½œç”¨æœºåˆ¶               |
|-----------------|------------|----------------------|
| ä¸Šä¸‹æ–‡é•¿åº¦       | 256 tokens | æ§åˆ¶æ¨¡å‹è®°å¿†å®¹é‡       |
| åµŒå…¥ç»´åº¦         | 384        | ç‰¹å¾è¡¨å¾ç©ºé—´å¤æ‚åº¦     |
| æ³¨æ„åŠ›å¤´æ•°       | 6          | å¹¶è¡Œç‰¹å¾æå–èƒ½åŠ›       |
| æ®‹å·®ä¸¢å¼ƒç‡       | 0.2        | é˜²æ­¢è¿‡æ‹Ÿåˆæ­£åˆ™åŒ–æ‰‹æ®µ   |
| å­¦ä¹ ç‡è¡°å‡ç­–ç•¥   | cosine     | ä¼˜åŒ–è®­ç»ƒç¨³å®šæ€§         |

åœ¨Lambda GPUå®ä¾‹ä¸Šï¼Œå®Œæ•´è®­ç»ƒå‘¨æœŸçº¦15åˆ†é’Ÿå¯è·å¾—éªŒè¯æŸå¤±1.48ï¼Œç”Ÿæˆæ–‡æœ¬å·²å…·å¤‡åŸºæœ¬è¯­æ³•ç»“æ„[2]ã€‚è¯¥å®ç°ç‰¹åˆ«è®¾è®¡äº†åˆ†é˜¶æ®µè®­ç»ƒç­–ç•¥ï¼šé¢„è®­ç»ƒé˜¶æ®µä½¿ç”¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™ï¼ˆçº¦300B tokensï¼‰ï¼Œå¾®è°ƒé˜¶æ®µé€šè¿‡æŒ‡ä»¤ä¼˜åŒ–å®ç°å¯¹è¯èƒ½åŠ›ã€‚

## å…³é”®æŠ€æœ¯ç»„ä»¶æ¼”è¿›

### 1. è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–è·¯å¾„

```mermaid
graph LR
A[æœ´ç´ æ³¨æ„åŠ›O(nÂ²)] --> B[åˆ†å—è®¡ç®—ä¼˜åŒ–]
B --> C[FlashAttentionç®—æ³•]
C --> D[å¤šå¤´å¹¶è¡Œå¤„ç†]
D --> E[KV Cacheæ¨ç†åŠ é€Ÿ]
```

è¯¥æ¼”è¿›è·¯çº¿ä½¿è®¡ç®—å¤æ‚åº¦ä»åŸå§‹Transformerçš„O(nÂ²)é™ä½è‡³å®é™…å¯æ¥å—çš„O(n log n)[2]ã€‚è§†é¢‘ä¸­é€šè¿‡çŸ©é˜µä¹˜æ³•å¯è§†åŒ–å±•ç¤ºäº†æ³¨æ„åŠ›æƒé‡å¦‚ä½•åŠ¨æ€èšç„¦å…³é”®ä¸Šä¸‹æ–‡ä½ç½®ã€‚

### 2. ä½ç½®ç¼–ç æ–¹æ¡ˆå¯¹æ¯”

æ–¹æ¡ˆ | å…¬å¼ | ä¼˜ç‚¹ | å±€é™æ€§
---|---|---|---
ç»å¯¹ä½ç½®ç¼–ç  | $$ PE(pos,2i) = \sin(pos/10000^{2i/d}) $$ | æ˜ç¡®ä½ç½®ä¿¡æ¯ | é•¿åº¦å¤–æ¨èƒ½åŠ›å·®
ç›¸å¯¹ä½ç½®ç¼–ç  | $$ a_{ij} = x_iW^Q(W^K)^Tx_j + x_iW^Qr_{i-j} $$ | æ›´å¥½æ•æ‰å±€éƒ¨å…³ç³» | å®ç°å¤æ‚åº¦é«˜
æ—‹è½¬ä½ç½®ç¼–ç  | $$ q_m^Tk_n = Re[ e^{i(mÎ¸ - nÎ¸)} q_m^Tk_n ] $$ | ä¿æŒå†…ç§¯ç¨³å®šæ€§ | éœ€è¦å®šåˆ¶åŒ–å®ç°

å®éªŒæ•°æ®æ˜¾ç¤ºï¼Œåœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰å¯ä½¿å›°æƒ‘åº¦é™ä½çº¦15%[2]ã€‚

## å®è·µè®­ç»ƒå»ºè®®

### 1. ç¡¬ä»¶èµ„æºé…ç½®ç­–ç•¥

```python
# åˆ†å¸ƒå¼è®­ç»ƒé…ç½®ç¤ºä¾‹
parallel_config = {
    "tensor_parallel_degree": 8,  # æ¨¡å‹å¹¶è¡Œç»´åº¦
    "pipeline_parallel_degree": 4, # æµæ°´çº¿å¹¶è¡Œç»´åº¦ 
    "micro_batch_size": 16,        # å¾®æ‰¹æ¬¡å¤§å°
    "gradient_accumulation_steps": 32
}
```

### 2. è°ƒè¯•ä¸ä¼˜åŒ–æŠ€æœ¯
- **æ¢¯åº¦è£å‰ª**ï¼šè®¾ç½®é˜ˆå€¼`max_grad_norm=1.0`é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šä½¿ç”¨`torch.cuda.amp`æ¨¡å—èŠ‚çº¦æ˜¾å­˜40%
- **æ¿€æ´»æ£€æŸ¥ç‚¹**ï¼šé€šè¿‡`torch.utils.checkpoint`é™ä½æ˜¾å­˜æ¶ˆè€—50%
- **å­¦ä¹ ç‡æ¢æµ‹**ï¼šæ‰§è¡Œçº¿æ€§æ‰«ææ‰¾å‡ºæœ€ä¼˜åˆå§‹å­¦ä¹ ç‡

åœ¨nanoGPTé¡¹ç›®ä¸­ï¼Œå¼•å…¥LayerScaleæŠ€æœ¯ï¼ˆæ¯å±‚è¾“å‡ºä¹˜ä»¥å¯å­¦ä¹ æ ‡é‡ï¼‰å¯ä½¿è®­ç»ƒç¨³å®šæ€§æå‡30%[2]ã€‚

## æ‰©å±•åº”ç”¨åœºæ™¯

### 1. ä»£ç ç”Ÿæˆä¼˜åŒ–æ¡ˆä¾‹
```python
def code_completion(prompt: str, temperature=0.8):
    tokens = tokenizer.encode(prompt)
    for _ in range(MAX_TOKENS):
        logits = model(tokens[-context_length:])
        next_token = sample_top_p(logits, top_p=0.95)
        tokens.append(next_token)
        if next_token == EOS: break
    return tokenizer.decode(tokens)
```
è¯¥ç®—æ³•é‡‡ç”¨Top-pé‡‡æ ·ï¼ˆnucleus samplingï¼‰ç­–ç•¥ï¼Œåœ¨ä¿æŒç”Ÿæˆå¤šæ ·æ€§çš„åŒæ—¶å‡å°‘è¯­æ³•é”™è¯¯ã€‚åœ¨Pythonä»£ç è¡¥å…¨ä»»åŠ¡ä¸­ï¼Œç›¸æ¯”è´ªå©ªè§£ç å¯ä½¿æ­£ç¡®ç‡æå‡28%[1]ã€‚

## å­¦ä¹ è·¯çº¿å›¾

é˜¶æ®µ | å†…å®¹ | å»ºè®®æ—¶é•¿ | å…³é”®äº§å‡º
---|---|---|---
åŸºç¡€æŒæ¡ | microgradå®ç°/MLPè®­ç»ƒ | 20å°æ—¶ | æ‰‹å†™æ•°å­—è¯†åˆ«æ¨¡å‹
è¿›é˜¶å®è·µ | Transformerç¼–ç å™¨/è§£ç å™¨ | 40å°æ—¶ | æœºå™¨ç¿»è¯‘åŸå‹ç³»ç»Ÿ
ä¸“ä¸šæ·±åŒ– | åˆ†å¸ƒå¼è®­ç»ƒ/RLHF | 60å°æ—¶ | é¢†åŸŸä¸“ç”¨å¯¹è¯ç³»ç»Ÿ
ç”Ÿäº§éƒ¨ç½² | æ¨¡å‹é‡åŒ–/æœåŠ¡åŒ– | 30å°æ—¶ | REST APIæœåŠ¡ç«¯ç‚¹

å»ºè®®é…åˆã€ŠThe Annotated Transformerã€‹ç­‰å¼€æºé¡¹ç›®è¿›è¡Œå¯¹ç…§å­¦ä¹ ï¼Œå¹¶åœ¨Kaggleå¹³å°å‚ä¸LLMç›¸å…³ç«èµ›ä»¥å·©å›ºæŠ€èƒ½[1][3]ã€‚

Citations:
[1] https://github.com/chizkidd/Karpathy-Neural-Networks-Zero-to-Hero
[2] https://www.youtube.com/watch?v=kCc8FmEb1nY
[3] https://karpathy.ai/zero-to-hero.html
[4] https://karpathy.ai
[5] https://www.reddit.com/r/learnmachinelearning/comments/17bmpy7/decreasing_loss_with_incorrectly_applied_softmax/
[6] https://www.reddit.com/r/learnmachinelearning/comments/1dsg6mi/those_who_loved_andrej_karpathys_zero_to_hero/
[7] https://podwise.ai/dashboard/collections/14
[8] https://news.ycombinator.com/item?id=34591998
[9] https://www.youtube.com/watch?v=F53Tt_vNLdg
[10] https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ
[11] https://www.youtube.com/andrejkarpathy
[12] https://github.com/karpathy/llm.c
[13] https://github.com/karpathy/nanoGPT
[14] https://news.ycombinator.com/item?id=40502693