# Andrej Karpathy's LLM Zero to Hero Series

## Course Overview

- **Prerequisites**: 
- **Programming Language**: Python
- **Difficulty Level**: üåü
- **Estimated Study Time**: 50 hours

## Course Resources

- **Course Videos**: [YouTube Channel](https://www.youtube.com/@AndrejKarpathy)
- **GitHub Repositories**:
  - [nanoGPT](https://github.com/karpathy/nanoGPT)
  - [llm.c](https://github.com/karpathy/llm.c)

## Core Lecture Series

### 1. Introductory Lectures on LLMs

#### [Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g)
- **Key Concepts**:
  - LLM definition and components
  - Training process (data requirements, computational resources)
  - Model capabilities and limitations
  - Safety challenges (jailbreak attacks, prompt injection)

#### [Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI)
- **Key Concepts**:
  - Pretraining stage (data collection, tokenization)
  - Neural network architecture
  - Inference process
  - Reinforcement learning fine-tuning

#### [How I use LLMs](https://www.youtube.com/watch?v=EWvNQjAaOHw)
- **Practical Applications**:
  - Writing and programming assistance
  - Multi-modal interactions
  - Custom instructions and memory features
  - Comparative analysis of different LLMs

### 2. Neural Networks: Zero to Hero

#### Building Blocks
- **Micrograd**: [2h25m] „Äê„ÄëIntroduction to neural networks and backpropagation](https://www.youtube.com/watch?v=VMj-3S1tku0&t=1s)
- **Makemore Series**:
  - [1h57m] Character-level language modeling
  - [1h15m] Multilayer perceptron implementation
  - [1h55m] Activations, gradients, and BatchNorm
  - [1h55m] Advanced backpropagation techniques
  - [56m] WaveNet architecture implementation

#### GPT Implementation
- **[Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)**:
  - Transformer architecture implementation
  - Attention mechanism explanation
  - Training process demonstration

- **[Reproducing GPT-2](https://www.youtube.com/watch?v=l8pRSuU81PU)**:
  - Full 124M parameter implementation
  - Training optimization techniques
  - Hyperparameter configuration
  - Results analysis

### 3. Tokenizer

#### [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)
- Byte Pair Encoding implementation
- Tokenizer training process
- Common tokenization issues
- Impact on model behavior

## Key Takeaways

- Provides hands-on implementation experience from basic neural networks to full LLMs
- Emphasizes understanding through building from scratch
- Covers both theoretical foundations and practical implementation details
- Includes modern architectures like Transformers and WaveNet
- Offers insights into training optimization and performance tuning