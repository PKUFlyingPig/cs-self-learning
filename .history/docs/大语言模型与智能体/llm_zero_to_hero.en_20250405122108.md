# Andrej Karpathy's LLM Zero to Hero Series

## Course Overview

- **Affiliation**: Independent (formerly OpenAI, Tesla)
- **Prerequisites**: 
  - Solid Python programming skills
  - Basic calculus knowledge (e.g., derivatives)
- **Programming Language**: Python
- **Difficulty Level**: ðŸŒŸ
- **Estimated Study Time**: 50-100 hours

## Course Resources

- **Course Videos**: [YouTube Channel](https://www.youtube.com/@AndrejKarpathy)
- **GitHub Repositories**:
  - [nanoGPT](https://github.com/karpathy/nanoGPT)
  - [llm.c](https://github.com/karpathy/llm.c)

## Core Lecture Series

### 1. Introductory Lectures on LLMs

#### [Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g)
- **Key Concepts**:
  - LLM definition and components
  - Training process (data requirements, computational resources)
  - Model capabilities and limitations
  - Safety challenges (jailbreak attacks, prompt injection)

#### [Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI)
- **Key Concepts**:
  - Pretraining stage (data collection, tokenization)
  - Neural network architecture
  - Inference process
  - Reinforcement learning fine-tuning

#### [How I use LLMs](https://www.youtube.com/watch?v=EWvNQjAaOHw)
- **Practical Applications**:
  - Writing and programming assistance
  - Multi-modal interactions
  - Custom instructions and memory features
  - Comparative analysis of different LLMs

### 2. Neural Networks: Zero to Hero

#### Building Blocks
- **Micrograd**: [2h25m] Introduction to neural networks and backpropagation
- **Makemore Series**:
  - [1h57m] Character-level language modeling
  - [1h15m] Multilayer perceptron implementation
  - [1h55m] Activations, gradients, and BatchNorm
  - [1h55m] Advanced backpropagation techniques
  - [56m] WaveNet architecture implementation

#### GPT Implementation
- **[Let's build GPT](https://www.youtube.com/watch?v=kCc8FmEb1nY)**:
  - Transformer architecture implementation
  - Attention mechanism explanation
  - Training process demonstration

- **[Reproducing GPT-2](https://www.youtube.com/watch?v=l8pRSuU81PU)**:
  - Full 124M parameter implementation
  - Training optimization techniques
  - Hyperparameter configuration
  - Results analysis

### 3. Special Topics

#### [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)
- Byte Pair Encoding implementation
- Tokenizer training process
- Common tokenization issues
- Impact on model behavior

## Learning Path Recommendations

1. **Beginners**:
   - Start with "Intro to Large Language Models"
   - Proceed through the "Neural Networks: Zero to Hero" series in order
   - Implement basic versions of each concept

2. **Intermediate Learners**:
   - Focus on the GPT implementation videos
   - Experiment with the nanoGPT repository
   - Study tokenization in depth

3. **Advanced Practitioners**:
   - Explore llm.c for optimized implementations
   - Modify architectures for specific use cases
   - Investigate performance optimization techniques

## Key Takeaways

- Provides hands-on implementation experience from basic neural networks to full LLMs
- Emphasizes understanding through building from scratch
- Covers both theoretical foundations and practical implementation details
- Includes modern architectures like Transformers and WaveNet
- Offers insights into training optimization and performance tuning